{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the following notebook is giving a brief introduction on the **Elastic Anonymization** algorithm fatures and usage. Briefly speaking, the algorithm works in six steps:\n",
    "1. Using a fine-tuned BERT to perform named entity recognition on sensitive information found in the corpus.\n",
    "2. The entities are, then, insterted back in the original corpus, as recognized by BERT.\n",
    "3. A Fast Text model is trained on the new corpus.\n",
    "4. For each new entity that needs to be anonymized, a similarity space is built as follows: the semantic and syntactic similarity between the entities and the other ones found in the corpus are computed and an anonymization region is defined in that space (for example, the square defined by $x=(.75, 1); y=(.75, 1)$).\n",
    "5. A DBSCAN algorithm is used to spot all the entities belonging to the anonymization region.\n",
    "6. All the spotted entities are anonymized using the same faking strategy.\n",
    "\n",
    "The entities and their respective fakings are then stored inside a dictionary which represents the mapping 1 on 1. Since similar entities will have the same faking, the keys will be unique, but the values won't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from transformers import pipeline\n",
    "\n",
    "from anonymizer.config.config import Config\n",
    "from anonymizer.preprocess.doc_processing import process\n",
    "from anonymizer.utils import (\n",
    "    create_documents_with_metadata, \n",
    "    visualize_ner_on_chunk\n",
    ")\n",
    "from anonymizer.elastic.eanon import ElasticAnonymizer\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVELS_RENAMER = {\n",
    "    \"level_0\": \"Macro area\",\n",
    "    \"level_1\": \"Tipo di documento\",\n",
    "    \"level_2\": \"Oggetto del documento\"\n",
    "}\n",
    "\n",
    "# the enonymize method from the ElasticAnonymizer class takes a list of langchain's Documents\n",
    "docs_ = process(Config.DOCS_PATH).rename(LEVELS_RENAMER, axis=1).fillna(\"\").head(5)\n",
    "docs = create_documents_with_metadata(docs_)\n",
    "# chunking is required from the BERT model to work properly (the inference breaks for long texts).\n",
    "chunker = SentenceTransformersTokenTextSplitter(chunk_overlap=50, tokens_per_chunk=250)\n",
    "chunks = chunker.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon = ElasticAnonymizer(use_pretrained_anon_state=False)\n",
    "anon_docs = anon.anonymize(chunks, show_ner=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central part of the elastic anonymization algorithm is building the similarity space for each entity. This space is defined by the **semantic** and **syntactic** similarity between an entity and all the other ones, recognized in the full corpus of selected documents. The two similarity measures are defined as follows:\n",
    "- **semantic**: the cosine similarity between the embedded entities, with the embeddings computed by a Fast Text model, trained on the entire corpus.\n",
    "- **syntactic**: the Jaro similarity between the entities.\n",
    "\n",
    "The main idea is that all the entities belonging to the \"anonymization region\" of the similarity space will be anonymized using the same fake entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the similarity space for the entity \"mooney\"\n",
    "anon.plot_anonimization_space(\n",
    "    \"p4i\", \n",
    "    add_labels=True, \n",
    "    add_anonymization_region=True, \n",
    "    anon_region_x=(.80, 1.02),\n",
    "    anon_region_y=(.75, 1.02)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Anonymization for p4i: {anon.anon_state['p4i']}\", \n",
    "    f\"\\nAnonymization for p4i p4i: {anon.anon_state['p4i p4i']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=\"osiria/deberta-base-italian-uncased-ner\", aggregation_strategy=\"simple\")\n",
    "visualize_ner_on_chunk(chunks[0], ner_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anonymized Text:\")\n",
    "wrap_ = textwrap.fill(anon_docs[\"text\"][0], width=120)\n",
    "display(Markdown(f\"```\\n{wrap_}\\n```\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Denonymized Text:\")\n",
    "wrap = textwrap.fill(anon.deanonymize([anon_docs[\"text\"][0]])[0], width=120)\n",
    "display(Markdown(f\"```\\n{wrap}\\n```\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of improvement possibilities here: the de-anonymization step is just a reverse mapping from the values to the keys of the anonymization state (the dictionary storing the entities and their corresponding fake), and they're recognized from the text using regular expressions. Smarter ways of doing that are surely available.\n",
    "\n",
    "Another improvement possibility concerns the inference time. In this example, we have relatively long documents (the GRC offering end proposals), with 5 documents resulting in 119 chunks. Still, the inference time was significant: 1 minute and 8 seconds including the training of the Fast Text model, which is usually very fast. A first step of improvement could be running the inference on the GPU. This should significantly reduce the inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
